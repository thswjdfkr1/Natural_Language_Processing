{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPmAQb0985Y6in7fsIxEG+h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SR72VFqgov-C"},"outputs":[],"source":["# 자연어 생성 모델의 경우 어떤 방식으로 생성하느냐에 따라 성능 차이가 커짐\n","# 자연어 생성 모델의 생성 방법들에 대해 이해하고 실제 적용 및 응용"]},{"cell_type":"markdown","source":["# 모델 불러오기"],"metadata":{"id":"Oj1ObYQoo8Ew"}},{"cell_type":"code","source":["!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n","!apt-get install git-lfs\n","!git lfs install\n","!git clone https://huggingface.co/taeminlee/kogpt2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0yy0dke7o3ph","executionInfo":{"status":"ok","timestamp":1742276134733,"user_tz":-540,"elapsed":58181,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"a64d7a6a-31e0-47d9-9553-f8f379411ce4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Detected operating system as Ubuntu/jammy.\n","Checking for curl...\n","Detected curl...\n","Checking for gpg...\n","Detected gpg...\n","Detected apt version as 2.4.13\n","Running apt-get update... done.\n","Installing apt-transport-https... done.\n","Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n","Importing packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\n","done.\n","Running apt-get update... done.\n","\n","The repository is setup! You can now install packages.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following packages will be upgraded:\n","  git-lfs\n","1 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n","Need to get 8,489 kB of archives.\n","After this operation, 7,671 kB of additional disk space will be used.\n","Get:1 https://packagecloud.io/github/git-lfs/ubuntu jammy/main amd64 git-lfs amd64 3.6.1 [8,489 kB]\n","Fetched 8,489 kB in 0s (26.9 MB/s)\n","(Reading database ... 125048 files and directories currently installed.)\n","Preparing to unpack .../git-lfs_3.6.1_amd64.deb ...\n","Unpacking git-lfs (3.6.1) over (3.0.2-1ubuntu0.3) ...\n","Setting up git-lfs (3.6.1) ...\n","Git LFS initialized.\n","Processing triggers for man-db (2.10.2-1) ...\n","Git LFS initialized.\n","Cloning into 'kogpt2'...\n","remote: Enumerating objects: 56, done.\u001b[K\n","remote: Total 56 (delta 0), reused 0 (delta 0), pack-reused 56 (from 1)\u001b[K\n","Unpacking objects: 100% (56/56), 1.53 MiB | 433.00 KiB/s, done.\n","Filtering content: 100% (3/3), 1.41 GiB | 56.97 MiB/s, done.\n"]}]},{"cell_type":"code","source":["import torch\n","from tokenizers import SentencePieceBPETokenizer\n","from transformers import GPT2Config, GPT2LMHeadModel\n","\n","tokenizer = SentencePieceBPETokenizer(\"/content/kogpt2/vocab.json\", \"/content/kogpt2/merges.txt\")\n","\n","config = GPT2Config(vocab_size=50000)\n","config.pad_token_id = tokenizer.token_to_id('<pad>')\n","model = GPT2LMHeadModel(config)\n","\n","model_dir = '/content/kogpt2/pytorch_model.bin'\n","\n","model.load_state_dict(torch.load(model_dir, map_location='cuda'), strict=False)\n","model.to('cuda')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LPBDnSuxo9oY","executionInfo":{"status":"ok","timestamp":1742276157465,"user_tz":-540,"elapsed":19317,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"fa4b5f02-832f-4e92-f68b-afe6d8a14857"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50000, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D(nf=2304, nx=768)\n","          (c_proj): Conv1D(nf=768, nx=768)\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D(nf=3072, nx=768)\n","          (c_proj): Conv1D(nf=768, nx=3072)\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",")"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["model.eval()\n","tokenizer.add_special_tokens([\"<s>\", \"</s>\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vef9fe51pY1z","executionInfo":{"status":"ok","timestamp":1742274602053,"user_tz":-540,"elapsed":40,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"604c9ad6-4166-4a2d-dcbc-8d2d0695c988"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["# Greedy Search\n"," 단순히 가장 높은 확률을 가진 단어를 다음 단어로 선택"],"metadata":{"id":"43negLSAqQA2"}},{"cell_type":"code","source":["def tokenizing(text):\n","    return torch.tensor(tokenizer.encode('<s> '+text, add_special_tokens=False).ids).unsqueeze(0).to('cuda')"],"metadata":{"id":"YbSQlV3pqHwB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = tokenizing(\"이순신은 조선 중기의 무신이다.\")\n","\n","# generate text until the output length (which includes the context length) reaches 100\n","# 생성 모델은 generate 함수를 통해 다음 token을 생성\n","\n","greedy_output = model.generate(input_ids, max_length=100)\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(greedy_output.tolist()[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F2-B7_ZJqhgJ","executionInfo":{"status":"ok","timestamp":1742276201650,"user_tz":-540,"elapsed":1806,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"9ba547fa-7d41-4275-c0cc-671e5f290594"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","<s> 이순신은 조선 중기의 무신이다.</s><s> 이 때문에 '이순신'은 '이순신'의 '이순신'으로, '이순신'은 '이순신'으로 각각 불린다.</s><s> '이순신'은 '이순신'으로, '이순신'은 '이순신'으로 각각 불린다.</s><s> '이순신'은 '이순신'으로, '이순신'은 '\n"]}]},{"cell_type":"code","source":["# 생성된 단어의 문맥은 합리적이지만 출력을 살펴보면 비슷한 단어가 계속 반복되는 문제가 보임\n","\n","# Greedy 탐색의 주요 단점은 낮은 확률 단어 이후에 나올수 있는 더 높은 확률의 단어를 놓친다는 점"],"metadata":{"id":"LqFY9FbCq1uE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Beam Search\n"," 각 단계에서 가장 확률이 높은 단어들의 num beams(정해진 개수)를 유지하고 결국 전체 확률이 가장 높은 순서를 선택하는 방법"],"metadata":{"id":"LULKKegNqvfN"}},{"cell_type":"code","source":["beam_output = model.generate(\n","    input_ids,\n","    max_length=50,\n","    num_beams=5,\n","    early_stopping=True\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1BhmRCPJqpGw","executionInfo":{"status":"ok","timestamp":1742276206332,"user_tz":-540,"elapsed":833,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"a4b710db-0ce5-41b6-fb71-f9f615c925f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","<s> 이순신은 조선 중기의 무신이다.</s><s> (서울= 뉴스와이어) 2012년 04월 05일 -- 국토해양부(장관 : 권도엽)는 2012년 4월 10일, 국토해양부(장관 : 권도엽)는\n"]}]},{"cell_type":"code","source":["# 동일한 단어가 계속해서 반복되는 현상이 동일하게 발생\n","\n","# 이러한 현상은 일반적인 언어생성 모델에서 나타나는 공통적인 문\n","\n","# 단순한 해결법은 n-gram 패널티를 도입하는 것\n","\n","# \"no_repeat_ngram_size=2\" 로 설정을 하면 동일 단어가 2번 이상 나타나는 것을 방지할 수 있음"],"metadata":{"id":"vRcqvG5or3pZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["beam_output = model.generate(\n","    input_ids,\n","    max_length=50,\n","    num_beams=5,\n","    early_stopping=True,\n","    no_repeat_ngram_size=2\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2tdfVZijsBFr","executionInfo":{"status":"ok","timestamp":1742276210487,"user_tz":-540,"elapsed":557,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"7de12aaa-5fde-4931-ecbe-ee86be599b8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","<s> 이순신은 조선 중기의 무신이다.</s><s> 그 후, 이조참의(吏曹參議)가 되고, 의정부영의정(議政府儀政)을 거쳐, 영의정이 되고 의정부좌찬성에 이르렀다.\n"]}]},{"cell_type":"code","source":["# Beam 탐색의 또 다른 특징으로 여러 후보들을 생성할 수 있음\n","\n","# \"num_return_sequences=5\"라는 설정을 하면, 총 5개의 생성 결과를 받아 봄\n","\n","# 단 num_return_sequences는 당연히 num_beams보다 작거나 같은 숫자이어야함"],"metadata":{"id":"jX2TjFiQsFTJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["beam_outputs = model.generate(\n","    input_ids,\n","    max_length=50,\n","    num_beams=5,\n","    no_repeat_ngram_size=2,\n","    num_return_sequences=5,\n","    early_stopping=True\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","for i, beam_output in enumerate(beam_outputs):\n","  print(\"{}: {}\".format(i, tokenizer.decode(beam_output.tolist(), skip_special_tokens=True)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5LTBy6MAsZ34","executionInfo":{"status":"ok","timestamp":1742276215101,"user_tz":-540,"elapsed":555,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"72926124-695c-4581-fe06-30a1ce75a0e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","0: <s> 이순신은 조선 중기의 무신이다.</s><s> 그 후, 그 해 11월 16일(음력 9월 15일)에 향년 60세를 일기로 사망하였다. &lt;삼국사기&gt;에 따르면, 그는 《\n","1: <s> 이순신은 조선 중기의 무신이다.</s><s> 그 후, 그 해 11월 16일(음력 9월 15일)에 향년 60세를 일기로 사망하였다. &lt;삼국사기&gt;에 따르면, 신라 경\n","2: <s> 이순신은 조선 중기의 무신이다.</s><s> 그 후, 그 해 11월 16일(음력 9월 15일)에 향년 60세를 일기로 사망하였다. &lt;삼국사기&gt;에 따르면, 신라에\n","3: <s> 이순신은 조선 중기의 무신이다.</s><s> 그 후, 그 해 11월 16일(음력 9월 15일)에 향년 60세를 일기로 사망하였다. &lt;삼국사기&gt;에 따르면, 그의 아들\n","4: <s> 이순신은 조선 중기의 무신이다.</s><s> 그 후, 그 해 11월 16일(음력 9월 15일)에 향년 60세를 일기로 사망하였다. &lt;삼국사기&gt;에 따르면, 그가 죽\n"]}]},{"cell_type":"code","source":["# 개방형 생성에서는 Beam 탐색이 최선이 아닐 수 있는 이유\n","\n","# 문장 생성 길이가 예측 가능한 태스크에서 잘 동작할 수 있지만, 대화 같이 길이가 유동적인 태스크에서는 잘 작동하지 않음.\n","# 반복 생성 문제에 취약\n","# 고품질 인간 언어는 높은 확률의 다음 단어 분포를 따르지 않음"],"metadata":{"id":"MTGxarhas0S5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Sampling\n","\n","사람의 언어를 보면 무조건 높은 확률을 선택하지 않기 때문에, 생성에 랜덤성을 부여할 필요가 있음\n","\n","그래서 제시된 방법이 Sampling\n","\n","가장 기본적인 형태의 Sampling은 조건부 확률 분포에 따라 다음 단어를 무작위로 선택하는 것을 의미"],"metadata":{"id":"e2tr4Uf8tTAs"}},{"cell_type":"code","source":["sample_output = model.generate(\n","    input_ids,\n","    do_sample=True, # 완전 random sampling\n","    max_length=50,\n","    top_k=0 # w/o top_k 추출 Top-K sampling을 비활성화\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KkxOhSy5tT4h","executionInfo":{"status":"ok","timestamp":1742276219390,"user_tz":-540,"elapsed":471,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"0fd175f6-7a52-4495-c07e-fa6286a9fb1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","<s> 이순신은 조선 중기의 무신이다.</s><s> 정보는 ‘왕의 정원’이라는 레이건 일가의 못된 마음을 바로잡는 데 공로를 세웠다는 평을 받는다.</s><s> 이 문서에서 그는 그는 탬파 대국회로 가민(嘉\n"]}]},{"cell_type":"code","source":["# 본문은 괜찮아 보이지만, 자세히 보면 일관성이 거의 없음\n","\n","# 이 문제를 보완하기 위한 한가지 트릭은 소프트맥스의 temperature를 낮추어 분포를 더 선명하게 만드는 것\n","\n","# 높은 확률의 단어의 가능성은 증가시키고, 낮은 확률의 단어 가능성은 감소시키는 효과를 줄 수 있음"],"metadata":{"id":"GaSaN7pDtynw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_output = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_k=0,\n","    temperature=0.7\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-eXzxpYOt_7Y","executionInfo":{"status":"ok","timestamp":1742276220082,"user_tz":-540,"elapsed":686,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"ef0cb2f0-6dcb-4b68-8762-253386118d7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","<s> 이순신은 조선 중기의 무신이다.</s><s> 영국왕립군</s><s> 영국왕립군()은 영국의 국왕을 보좌하는 국체로, 영국의 행정기구로, 영국 제2대 국왕인 영국의 엘리자베스 2세 여왕(재위\n"]}]},{"cell_type":"code","source":["# temperature를 적용해 분포가 덜 랜덤해 졌지만, 0으로 설정해버리면 Greedy 탐색과 동일해져버림"],"metadata":{"id":"gdRMQGvZuAEn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Top-K sampling\n","\n","K=6으로 설정을 하면, Sampling pool이 확률이 가장 높은 6개의 단어로 제한.\n","\n","첫 step에서는 0.68로 일부분에서만 디코딩 되고, 두번째 step에서는 0.99까지 올라감\n","\n","하지만, 두번째 샘플링에서 \"not\", \"the\", \"small\", \"told\"와 같이 이상한 후보들이 잘 제거됨\n","\n","이러한 방식으로 일관성 있으면서, 랜덤성을 부여하는게 가능"],"metadata":{"id":"az8jW3FWuJXe"}},{"cell_type":"code","source":["sample_output = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_k=50\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"edeUdo3YuLRv","executionInfo":{"status":"ok","timestamp":1742276223040,"user_tz":-540,"elapsed":425,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"0c16dc1f-9897-4136-9069-b3fcf203981a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","<s> 이순신은 조선 중기의 무신이다.</s><s> 하지만 3명의 주사가 각각 1대 1로 대치된 상황으로, 2교시 5분에 두 개의 주사가 1대 1로 대치되다가 4교시 10분(각 1대\n"]}]},{"cell_type":"markdown","source":["# Top-p (nucleus) sampling\n","\n","확률이 가장높은 K개의 단어가 아니라, 누적확률이 P를 초과하는 최소한의 단어 집합에서 샘플링을 진행하는 방식\n","\n","이렇게 하면 필터링된 단어수 또한 동적으로 변화가 가능"],"metadata":{"id":"tiYrUba3ui1h"}},{"cell_type":"code","source":["sample_output = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_p=0.92,\n","    top_k=0\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cCGMM9rGuZH3","executionInfo":{"status":"ok","timestamp":1742276227450,"user_tz":-540,"elapsed":553,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"be2d40fe-59b1-499c-a260-1d6d5c6571e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","<s> 이순신은 조선 중기의 무신이다.</s><s> 박노자 본명 '논어'는 줄임말 '문(文)'이 있지, 듣기만 해보면 속담만쓰게 돼 있다.</s><s> 청중 앞에서 참\n"]}]},{"cell_type":"code","source":["# 이론적으로는 top-p가 top-k보다 더 성능이 좋을 것 같지만, 실제로는 크게 다르지 않음\n","\n","# top-p와 top-k는 동시에 사용할 수도 있음"],"metadata":{"id":"ii6_37-zvgkZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["beam_outputs = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=100,\n","    top_k=50,\n","    top_p=0.95,\n","    num_return_sequences=5\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","for i, beam_output in enumerate(beam_outputs):\n","  print(\"{}: {}\".format(i, tokenizer.decode(beam_output.tolist(), skip_special_tokens=True)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pT21IFmHvAc0","executionInfo":{"status":"ok","timestamp":1742276230908,"user_tz":-540,"elapsed":1425,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"74f037f3-04ff-4041-a5aa-99955d6caf36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","0: <s> 이순신은 조선 중기의 무신이다.</s><s> 그는 《신증동국여지승람》, 《경국대전》, 《유월지절》, 《조선지신론》 3권을 통해 유학의 개념을 체계화하고, 그 이론을 토대로 하여, 조선 성리학의 중요한 학술적 근거로 삼은 저작이다.</s><s> 그는 《유월지절》, 《경국대전》, 《유월지절》 등을 통해 성리학의 근본개념, 방법론, 학문, 그리고 성리학의\n","1: <s> 이순신은 조선 중기의 무신이다.</s><s> ⊙ 제15대 서울지방법원장 이상욱(사법시험 20회)</s><s> ▶ 더 빠르고 편리하게 만나는 실시간 모바일 뉴스</s><s> 최필립, 이사장 사퇴 요구..</s><s> 이사장직 사퇴 의사표명 - 중앙일보 뉴스</s><s> 최필립, 이사장직 사퇴 요구..</s><s> 이사장직 사퇴 의사표명 - 중앙일보 뉴스</s><s> 5일 정치권에 따르면 최필립 이사장은 최근 언론 인터뷰를 통해 자신의 이사장으로 취임하는 것이 한국 사회의 발전에\n","2: <s> 이순신은 조선 중기의 무신이다.</s><s> 제16장: <남정>에서 말하는 정사는(정) 1.</s><s> 조선은 18년(1836) 7월 2일(조선 고종 1년 3월 2일)에 일본과의 외교관계를 수립하였다.</s><s> 제174년(조선 고종 23년 2월 1일) 10일(조선 고종 4년 12월 16일)까지 일본에 파견되어 있던 일본군 5천 명과 일본으로부터 외교적인\n","3: <s> 이순신은 조선 중기의 무신이다.</s><s> 이 중 한사람은 \"김 선생이 이 땅의 민주주의와 국민주권을 지키기 위해 헌신하신 것을 잊지 않고 있다\"면서 \"역사의 물길을 따라 걸어갔을 때, 이 땅의 자유와 평화가 더 이상 헛되지 않을 것이라고 확신한다\"고 말했다.</s><s> 김 선생은 광복 후 이 땅의 민주화를 위해 헌신하다가 1953년 11월 서울 종로구 효창동 자택에서 의문사한 뒤 의문사한 것으로 추정되고 있다.</s><s> 김 선생은 1962\n","4: <s> 이순신은 조선 중기의 무신이다.</s><s> 《홍무제전》(洪武帝全)의 《홍무제전(洪武帝戰)》은 삼국시대에 편찬된 《홍무제전》(洪武帝全)을 말한다.</s><s> 《홍무제전》은 신라와 조선에 걸쳐 두 국가의 통일 이후 통일한 국가의 통일상을 정리한 역사서이다.</s><s> 《홍무제전》은 조선시대 삼국시대에는 세조로부터 신라와\n"]}]},{"cell_type":"code","source":["# ad-hoc decoding방법에 따르면 Top-p와 Top-k sampling은 기존 Greedy-, Beam search 보다 개방형 언어생성에서 더 유창한 문장을 생성하는 것으로 보임\n","# 그러나 Top-p와 Top-k sampling 또한 Greedy-, Beam search 처럼 반복적 Word sequencd에 대한 문제가 발생\n","# 논문에 따르면 Beam search가 모델 트레이닝 목적함수를 잘 조정한다면 Top-p보다 더 유찬한 텍스트를 생선한다는 연구도 있음\n","# 개방형 언어생성은 빠르게 발전하는 분야이며, 무엇이 적합하다고 단정할 수 없으므로 특정 사용 사례에서 가장 잘 작동하는 방법이 무엇인지 고려\n","# 언어별, 모델별로 최적의 생성 방식을 찾아내야 함\n"],"metadata":{"id":"PPaHzhTvuoxw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dhhGa4dBvMqA"},"execution_count":null,"outputs":[]}]}